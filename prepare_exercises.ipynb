{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ce7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    " #nltk.download('all')\n",
    "    #nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e2c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = pd.read_csv('blog_articles.csv')\n",
    "news_df = pd.read_csv('news_articles.csv')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f713971a",
   "metadata": {},
   "source": [
    "Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "Lowercase everything\n",
    "Normalize unicode characters\n",
    "Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "raw",
   "id": "817131f1",
   "metadata": {},
   "source": [
    "Coming into our Data Science program, you will need to know some math and stats. However, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! Data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. But what “skills” do we mean, e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e66d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Coming into our Data Science program, you will need to know some math and stats. However, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! Data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. But what “skills” do we mean, e'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006fe934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make text all lower case \n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701ee52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming into our data science program, you will need to know some math and stats. however, many of our applicants actually learn in the application process  you dont need to be an expert before applying! data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. but what skills do we mean, e'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize unicode characters\n",
    "text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afc848f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process  you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean e'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove anything that is not a number, letter, white space or single quote \n",
    "text = re.sub('[^a-z0-9\\'\\s]', '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbeb8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basic clean function\n",
    "def basic_clean(text):\n",
    "    '''this function will produce cleaned string'''\n",
    "    # make text all lower case \n",
    "    text = text.lower()\n",
    "    #normalize unicode characters \n",
    "    text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    # remove anything that is not a number, letter, white space or single quote \n",
    "    text = re.sub('[^a-z0-9\\'\\s]', '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38385b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process  you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean e'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function\n",
    "basic_clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b3472b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean e\n"
     ]
    }
   ],
   "source": [
    "#tokenize string, return str false will return words in list\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "print(tokenizer.tokenize(text,return_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7711b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create define tokenize function\n",
    "def tokenize(text):\n",
    "    '''this function will tokenize string'''\n",
    "    #tokenize string, return str false will return words in list\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    print(tokenizer.tokenize(text,return_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab5d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean e\n"
     ]
    }
   ],
   "source": [
    "# testing function\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9472d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the stem\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88a3159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come into our data scienc program you will need to know some math and stat howev mani of our applic actual learn in the applic process you dont need to be an expert befor appli data scienc is a veri access field to anyon dedic to learn new skill and we can work with ani applic to help them learn what they need to know but what skill do we mean e'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through words in string and apply stem\n",
    "stems = [ps.stem(word) for word in text.split()]\n",
    "' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "313228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    # create the stem\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # loop through words in string and apply stem\n",
    "    stems =[ps.stem(word) for word in text.split()]\n",
    "    # returns stems joined in one paragraph\n",
    "    text = ' '.join(stems)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4959039a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come into our data scienc program you will need to know some math and stat howev mani of our applic actual learn in the applic process you dont need to be an expert befor appli data scienc is a veri access field to anyon dedic to learn new skill and we can work with ani applic to help them learn what they need to know but what skill do we mean e'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing stem function\n",
    "stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16839c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create object to hold lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "055c053c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming into our data science program you will need to know some math and stats however many of our applicant actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skill and we can work with any applicant to help them learn what they need to know but what skill do we mean e'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through words in string and apply lemmatizer\n",
    "lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "#returns lemmas joined \n",
    "' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a37aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    #create object to hold lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # loop through words in string and apply lemmatizer\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    #returns lemmas joined \n",
    "    text = ' '.join(lemmas)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "655c9ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming into our data science program you will need to know some math and stats however many of our applicant actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skill and we can work with any applicant to help them learn what they need to know but what skill do we mean e'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function test \n",
    "lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59b36c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords in english default\n",
    "stopwords_text = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "429d3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits text into list \n",
    "words = text.split()\n",
    "# filtering out words that user does not want\n",
    "filtered_words = [word for word in text.split() if word not in stopwords_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54234ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 35 stopwords\n"
     ]
    }
   ],
   "source": [
    "# printing number of stopwords removed \n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f38f638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming data science program need know math stats however many applicants actually learn application process dont need expert applying data science accessible field anyone dedicated learning new skills work applicant help learn need know skills mean e'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text without stop words \n",
    "text_without_stopwords = ' '.join(filtered_words)\n",
    "text_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21fa06f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# learning about sets \n",
    "\n",
    "list1 = [1, 2, 3, 4]\n",
    "list2 = [2, 1, 3, 4]\n",
    "\n",
    "print(set(list1)==set(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e78b0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 == list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e59ceb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'c', 'd'] {'a', 'c', 'd', 'b'}\n"
     ]
    }
   ],
   "source": [
    "mylist = ['a', 'b', 'c', 'c', 'd']\n",
    "\n",
    "myset = set(mylist)\n",
    "\n",
    "print(mylist, myset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96f13f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords function \n",
    "# ignore means ignore if you can't read it then pass through\n",
    "def remove_stopwords(text, extra_words = [], exclude_words = []):\n",
    "    '''this function will remove stopwords in string \n",
    "    and return string without stop words'''\n",
    "    # assign our stopwords from nltk into stopword_list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # utilizing set casting, i will remove any excluded stopwords\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    # add in any extra words to my stopwords set using a union\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    # split our document by spaces\n",
    "    words = text.split()\n",
    "    # every word in our document, as long as that word is not in our stopwords\n",
    "    filtered_words = [word for word in text if word not in stopword_list]\n",
    "    # glue it back together with spaces, as it was so it shall be\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    # return the document back\n",
    "    return string_without_stopwords\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4813c5f0",
   "metadata": {},
   "source": [
    "using set makes order not matter, cares about content not order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf38c32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g     u r       p r g r   u   w l l       k w     h       h w v r     f   u r   p p l   u l l   l r     h   p p l   p r     u         b     x p r   b f r   p p l g           v r   b l   f l           l r g   w   k l l     w     w r k   w h     p p l     h l p   h   l r   w h   h       k w   b u   w h   k l l     w    '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "remove_stopwords(text,'science','is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18b4064e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acquire' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m news_df \u001b[38;5;241m=\u001b[39m \u001b[43macquire\u001b[49m\u001b[38;5;241m.\u001b[39mget_news_articles_data()\n\u001b[1;32m      2\u001b[0m news_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acquire' is not defined"
     ]
    }
   ],
   "source": [
    "news_df = acquire.get_news_articles_data()\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f449f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_blog_articles_data()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09024923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want:\n",
    "#  -clean: normalized/tokenized, with stopwords removed\n",
    "#         apply: basic_clean, tokenize, remove_stopwords\n",
    "#  -stemmed: stemmed version of cleaned data\n",
    "            # apply: stem function onto cleaned data\n",
    "#  -lemmatized: lemmatized version of cleaned data\n",
    "            # apply: lemmatize function onto cleaned datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85133fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['some_col'] = df['old_col'].apply(some_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ed93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'content': 'original'}, inplace=True)\n",
    "codeup_df.rename(columns={'content': 'original'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ea8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa734bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e24508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords,\n",
    "                                  extra_words=extra_words,\n",
    "                                  exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
